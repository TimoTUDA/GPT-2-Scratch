# GPT-2-Scratch
This repository holds a simple form-scratch implementation of GPT-2 (124M) model. 

The implementation starts from an empty file that is extended until the GPT-2 model is reproduced. With more training time and money the code can also reproduce GPT-3 models.

Please note that GPT-2 is just a simple language model that is trained on internet documents. This repository does not contain Chat finetunging, and it is not a Chat assistent.
For now, it will be prompted with "Hello, I'm a language model,". After 10B tokens of training it returns:
```
Hello, I'm a language model, and my goal is to make English as easy and fun as possible for everyone, and to find out the different grammar rules
Hello, I'm a language model, so the next time I go, I'll just say, I like this stuff.
Hello, I'm a language model, and the question is, what should I do if I want to be a teacher?
Hello, I'm a language model, and I'm an English person. In languages, "speak" is really speaking. Because for most people, there's
```

And after 40B tokens of training:

```
Hello, I'm a language model, a model of computer science, and it's a way (in mathematics) to program computer programs to do things like write
Hello, I'm a language model, not a human. This means that I believe in my language model, as I have no experience with it yet.
Hello, I'm a language model, but I'm talking about data. You've got to create an array of data: you've got to create that.
Hello, I'm a language model, and all of this is about modeling and learning Python. I'm very good in syntax, however I struggle with Python due
```

A very comprehensive and detailed explanation can be found at: https://www.youtube.com/watch?v=l8pRSuU81PU
